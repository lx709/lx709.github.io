---
permalink: /
title: "About me ([Curriculum Vitae](https://xiangli.ac.cn/files/xiang_en.pdf))"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

<!-- <p align="center">
  <img src="https://lx709.github.io/images/lx.jpg?raw=true" alt="Photo" style="width: 100px;"/> 
</p> -->

Dr. Xiang Li (personal website: https://xiangli.ac.cn) is a Lecturer in the Department of Computer Science at the University of Reading. His research focuses on multimodal large language models, computer vision, and remote sensing data processing. Dr. Li has published over 50 papers in top conferences and journals in computer vision and remote sensing, including CVPR, ICCV, NeurIPS, TVCG, and TGRS, with 5000+ citations on Google Scholar and an h-index of 25. He collaborates with leading researchers from institutions such as the Technical University of Munich, New York University, King Abdullah University of Science and Technology, Wuhan University, and the Aerospace Information Research Institute, Chinese Academy of Sciences. Dr. Li was involved in the MiniGPT-4 project (>25k stars on GitHub) and serves as a guest editor for the IEEE GRSM special issue on “Vision-Language Models in Remote Sensing.” His research aims to drive innovation in AI technologies for Earth observation and practical applications. Dr Li got his Bachelor’s degree from Wuhan University in 2014 and his PhD degree from the Aerospace Information Research Institute, Chinese Academy of Sciences in 2019.

<h2><span>Selected Publications</span></h2>
[# denotes equal contribution, * denotes corresponding author]
<table cellspacing="0" cellpadding="0">

<tr>
<td style="padding:0px;width:30%;vertical-align:middle">
  <img src="../images/minigpt4.png" height="100%" width="100%" style="border-style: none">
</td>
<td style="padding:20px;width:70%;vertical-align:middle">  
  <h3>MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models.</h3>
  Deyao Zhu, Jun Chen, Xiaoqian Shen, <b>Xiang Li</b>, Mohamed Elhoseiny.
  <br>
  <em>arxiv</em>, 2023
  <br>
  <div>
    <a href="https://minigpt-4.github.io/">[project]</a>
    <a href="https://arxiv.org/abs/2304.10592">[paper]</a>
    <a href="https://github.com/Vision-CAIR/MiniGPT-4">[code]</a>
    <a href="https://huggingface.co/spaces/Vision-CAIR/minigpt4">[huggingface demo]</a>
  </div>  
</td>
</tr>  

<tr>
<td style="padding:0px;width:30%;vertical-align:middle">
  <img src="../images/minigpt-v2.png" height="100%" width="100%" style="border-style: none">
</td>
<td style="padding:20px;width:70%;vertical-align:middle">  
  <h3>MiniGPT-v2: Large Language Model as a Unified Interface for Vision-Language Multi-task Learning.</h3>
  Jun Chen, Deyao Zhu, Xiaoqian Shen, <b>Xiang Li</b>, Zechun Liu, Pengchuan Zhang, Raghuraman Krishnamoorthi, Vikas Chandra, Yunyang Xiong, Mohamed Elhoseiny.
  <br>
  <em>arxiv</em>, 2023
  <br>
  <div>
    <a href="https://minigpt-v2.github.io/">[project]</a>
    <a href="https://arxiv.org/abs/2310.09478">[paper]</a>
    <a href="https://github.com/Vision-CAIR/MiniGPT-4">[code]</a>
    <a href="https://huggingface.co/spaces/Vision-CAIR/MiniGPT-v2">[huggingface demo]</a>
  </div>  
</td>
</tr>  


<tr>
<td style="padding:0px;width:30%;vertical-align:middle">
  <img src="../images/rsvlm.png" height="100%" width="100%" style="border-style: none">
</td>
<td style="padding:20px;width:70%;vertical-align:middle">  
  <h3>Vision-Language Models in Remote Sensing: Current Progress and Future Trends.
  </h3>
  <b>Xiang Li*</b>, Congcong Wen, Yuan Hu, Zhengpeng Yuan, Xiao Xiang Zhu.
  <br>
  <em>IEEE Geoscience and Remote Sensing Magazine (GRSM)</em>, 2024
  <br>
  <div>
    <!-- <a href="https://minigpt-4.github.io/">[project]</a> -->
    <a href="https://arxiv.org/abs/2305.05726">[paper]</a>
    <!-- <a href="https://github.com/Vision-CAIR/MiniGPT-4">[code]</a> -->
    <!-- <a href="https://huggingface.co/spaces/Vision-CAIR/minigpt4">[huggingface demo]</a> -->
  </div>  
</td>
</tr> 


<tr>
<td style="padding:0px;width:30%;vertical-align:middle">
  <img src="../images/rsgpt.png" height="100%" width="100%" style="border-style: none">
</td>
<td style="padding:20px;width:70%;vertical-align:middle">  
  <h3>RSGPT: A Remote Sensing Vision Language Model and Benchmark.</h3>
  Yuan Hu, Jianlong Yuan, Congcong Wen, Xiaonan Lu, <b>Xiang Li*</b>.
  <br>
  <em>arxiv</em>, 2023
  <br>
  <div>
    <a href="https://github.com/Lavender105/RSGPT">[project]</a>
    <a href="https://arxiv.org/abs/2307.15266">[paper]</a>
    <a href="https://github.com/Lavender105/RSGPT">[code]</a>
  </div>  
</td>
</tr>  

<tr>
<td style="padding:0px;width:30%;vertical-align:middle">
  <img src="../images/rsclip.png" height="100%" width="100%" style="border-style: none">
</td>
<td style="padding:20px;width:70%;vertical-align:middle">  
  <h3>RS-CLIP: Zero Shot Remote Sensing Scene Classification via Contrastive Vision-Language Supervision.</h3>
  <b>Xiang Li</b>, Xiang Li, Congcong Wen, Nan Zhou.
  <br>
  <em>International Journal of Applied Earth Observation and Geoinformation (JAG)</em>, 2023
  <br>
  <div>
    <a href="https://github.com/lx709/RS-CLIP">[project]</a>
    <a href="https://www.sciencedirect.com/science/article/pii/S1569843223003217">[paper]</a>
    <a href="https://github.com/lx709/RS-CLIP">[code]</a>
  </div>  
</td>
</tr>  



<tr>
<td style="padding:0px;width:30%;vertical-align:middle">
  <img src="../images/fsodm.png" height="100%" width="100%" style="border-style: none">
</td>
<td style="padding:20px;width:70%;vertical-align:middle">  
  <h3>Few-shot Object Detection on Remote Sensing Images.
  </h3>
  <b>Xiang Li<sup>#</sup></b>, Jingyu Deng<sup>#</sup>, Yi Fang.
  <br>
  <em>TGRS</em>, 2021
  <br>
  <div>
    <!-- <a href="https://github.com/xiaoqian-shen/MoStGAN-V/">[project]</a> -->
    <a href="https://ieeexplore.ieee.org/document/9362267">[paper]</a>
    <a href="https://github.com/lixiang-ucas/FSODM">[code]</a>
    <!-- <a href="https://huggingface.co/spaces/Vision-CAIR/minigpt4">[huggingface demo]</a> -->
  </div>  
</td>
</tr> 



<tr>
<td style="padding:0px;width:30%;vertical-align:middle">
  <img src="../images/dancenet.png" height="100%" width="100%" style="border-style: none">
</td>
<td style="padding:20px;width:70%;vertical-align:middle">  
  <h3>Density-Aware Convolutional Networks with Context Encoding for Airborne LiDAR Point Cloud Classification.
  </h3>
  <b>Xiang Li</b>, Lingjing Wang, Mingyang Wang, Congcong Wen, Nan Zhou, Yi Fang.
  <br>
  <em>ISPRS JPRS</em>, 2021
  <br>
  <div>
    <!-- <a href="https://github.com/xiaoqian-shen/MoStGAN-V/">[project]</a> -->
    <a href="https://arxiv.org/abs/1910.05909">[paper]</a>
    <!-- <a href="https://github.com/xiaoqian-shen/MoStGAN-V">[code]</a> -->
    <!-- <a href="https://huggingface.co/spaces/Vision-CAIR/minigpt4">[huggingface demo]</a> -->
  </div>  
</td>
</tr> 



<tr>
<td style="padding:0px;width:30%;vertical-align:middle">
  <img src="../images/tpnet.png" height="100%" width="100%" style="border-style: none">
</td>
<td style="padding:20px;width:70%;vertical-align:middle">  
  <h3>Topology Constrained Shape Correspondence.
  </h3>
  <b>Xiang Li<sup>#</sup></b>, Congcong Wen<sup>#</sup>, Lingjing Wang, Yi Fang.
  <br>
  <em>TVCG</em>, 2020
  <br>
  <div>
    <!-- <a href="https://github.com/xiaoqian-shen/MoStGAN-V/">[project]</a> -->
    <a href="https://ieeexplore.ieee.org/document/9091324">[paper]</a>
    <a href="https://github.com/lixiang-ucas/TP-Net">[code]</a>
    <!-- <a href="https://huggingface.co/spaces/Vision-CAIR/minigpt4">[huggingface demo]</a> -->
  </div>  
</td>
</tr> 


<tr>
<td style="padding:0px;width:30%;vertical-align:middle">
  <img src="../images/wps-net.png" height="100%" width="100%" style="border-style: none">
</td>
<td style="padding:20px;width:70%;vertical-align:middle">  
  <h3>Few-shot Learning of Part-specific Probability Space for 3D Shape Segmentation.
  </h3>
  Lingjing Wang<sup>#</sup>, <b>Xiang Li<sup>#</sup></b>, Yi Fang.
  <br>
  <em>CVPR</em>, 2020
  <br>
  <div>
    <!-- <a href="https://github.com/xiaoqian-shen/MoStGAN-V/">[project]</a> -->
    <a href="https://paperswithcode.com/paper/few-shot-learning-of-part-specific">[paper]</a>
    <a href="https://github.com/Lingjing324/Few-Shot-Learning-of-Part-Specific-Probability-Space-for-3D-Shape-Segmentation">[code]</a>
    <!-- <a href="https://huggingface.co/spaces/Vision-CAIR/minigpt4">[huggingface demo]</a> -->
  </div>  
</td>
</tr> 



<tr>
<td style="padding:0px;width:30%;vertical-align:middle">
  <img src="../images/d-fcn.png" height="100%" width="100%" style="border-style: none">
</td>
<td style="padding:20px;width:70%;vertical-align:middle">  
  <h3>Directionally Constrained Fully Convolutional Neural Network For Airborne Lidar Point Cloud Classification.
  </h3>
  Congcong Wen, Lina Yang, Ling Peng, <b>Xiang Li*</b>
  <br>
  <em>ISPRS JPRS</em>, 2020
  <br>
  <div>
    <!-- <a href="https://github.com/xiaoqian-shen/MoStGAN-V/">[project]</a> -->
    <a href="https://authors.elsevier.com/a/1abO93I9x1cfvT">[paper]</a>
    <a href="https://https://github.com/lx709/D-FCN">[code]</a>
    <!-- <a href="https://huggingface.co/spaces/Vision-CAIR/minigpt4">[huggingface demo]</a> -->
  </div>  
</td>
</tr> 


</table>


Honors and Awards
======
* Outstanding Reviewer for ICCV 2021.
* Postdoc Non-travel Award, NYUAD 2020 & 2021.
* National Scholarship, University of Chinese Academy of Sciences, 2018
* China Scholarship Council (CSC) scholarship, 2017
* Director's Fund of RADI, 2017
* Seagate Scholarship, Wuhan University, 2012
* National Scholarship, Wuhan University, 2011



