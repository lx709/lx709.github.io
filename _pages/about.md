---
permalink: /
title: "About me ([Curriculum Vitae](https://xiangli.ac.cn/files/xiang_en.pdf))"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

<!-- <p align="center">
 ¬†<img src="https://lx709.github.io/images/lx.jpg?raw=true" alt="Photo" style="width: 100px;"/> 
</p> -->

Dr. Xiang Li is a Lecturer in the Department of Computer Science at the University of Reading. His research focuses on multimodal large language models, computer vision, and remote sensing. Dr. Li has published over 50 papers in top conferences and journals in computer vision and remote sensing, including CVPR, ICCV, NeurIPS, TVCG, and TGRS, with 6500+ citations on Google Scholar and an h-index of 27. He collaborates with leading researchers from institutions such as the TUM, NYU, KAUST, WHU, and AIR CAS. Dr. Li contributed to the well-known MiniGPT-4 project, with over 2700 citations and 25k stars on GitHub. He served as a guest editor for the IEEE GRSM special issue on ‚ÄúVision-Language Models in Remote Sensing.‚Äù His research aims to drive innovation in AI technologies for Earth observation and practical applications. Dr Li got his Bachelor‚Äôs degree from Wuhan University in 2014 and his PhD degree from the Aerospace Information Research Institute, Chinese Academy of Sciences in 2019.

üî•Hiring PhD Students and Interns: welcome enthusiastic students passionate about computer vision, multimodal large language models, and remote sensing to apply for PhD positions and internships.

üî• News
* [03/2025] [RSGPT](https://github.com/Lavender105/RSGPT) was accepted for publication at ISPRS JPRS. RSGPT is the first attempt of GPT-based MLLMs in remote sensing.
* [03/2025] [Stable-SPAM](https://arxiv.org/abs/2502.17055) was accepted for publication at COLM. 
* [12/2024] Opening position: One fully funded PhD position in AI for Bioversity.
* [10/2024] I joined the University of Reading as a Lecturer in Computer Science.
* [09/2024] Two papers accepted at NeruIPS 2024 Datasets and Benchmarks Track [VRSBench](https://github.com/lx709/VRSBench) and [3DCoMPaT200](https://github.com/3DCoMPaT200/3DCoMPaT200).
* [07/2024] One paper on 3D LLM (Uni3DL) was accepted by ECCV 2024.
* [07/2024] I served as the Guest Editor for the special issue "[Vision-Language Models in Remote Sensing](https://www.grss-ieee.org/wp-content/uploads/2024/10/call-for-papers_GRSM_SI_VLM.pdf)" at IEEE GRSM.
* [06/2024] I'm co-organizing the [C3DV 2024](https://3dcompat-dataset.org/workshop/C3DV24/): 2nd Workshop on Compositional 3D Vision at CVPR 2024.
* [05/2024] One paper on few-shot object detection (InfRS) was accepted by TGRS.
* [05/2024] One paper on CO<sub>2</sub> mapping was published at JAG.
* [04/2024] Our survey paper [Vision-Language Models in Remote Sensing](https://ieeexplore.ieee.org/document/10506064) was published at IEEE GRSM.
* [11/2023] [RS-CLIP](https://github.com/lx709/RS-CLIP) was published at JAG.
* [10/2023] We released [MiniGPT-v2](https://minigpt-4.github.io/).
* [07/2023] We released [RSGPT](https://github.com/Lavender105/RSGPT), the first GPT-based multimodal LLM in remote sensing.
* [05/2023] We released the survey paper [Vision-Language Models in Remote Sensing(https://arxiv.org/abs/2305.05726).
* [04/2023] We released [MiniGPT-4](https://minigpt-4.github.io/).

<h2><span>Selected Publications</span></h2>
[# denotes equal contribution, * denotes corresponding author]
<table cellspacing="0" cellpadding="0">

<tr>
<td style="padding:0px;width:30%;vertical-align:middle">
  <img src="../images/minigpt4.png" height="100%" width="100%" style="border-style: none">
</td>
<td style="padding:20px;width:70%;vertical-align:middle">  
  <h3>MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models.</h3>
  Deyao Zhu, Jun Chen, Xiaoqian Shen, <b>Xiang Li</b>, Mohamed Elhoseiny.
  <br>
  <em>arxiv</em>, 2023
  <br>
  <div>
    <a href="https://minigpt-4.github.io/">[project]</a>
    <a href="https://arxiv.org/abs/2304.10592">[paper]</a>
    <a href="https://github.com/Vision-CAIR/MiniGPT-4">[code]</a>
    <a href="https://huggingface.co/spaces/Vision-CAIR/minigpt4">[huggingface demo]</a>
  </div>  
</td>
</tr>  

<tr>
<td style="padding:0px;width:30%;vertical-align:middle">
  <img src="../images/minigpt-v2.png" height="100%" width="100%" style="border-style: none">
</td>
<td style="padding:20px;width:70%;vertical-align:middle">  
  <h3>MiniGPT-v2: Large Language Model as a Unified Interface for Vision-Language Multi-task Learning.</h3>
  Jun Chen, Deyao Zhu, Xiaoqian Shen, <b>Xiang Li</b>, Zechun Liu, Pengchuan Zhang, Raghuraman Krishnamoorthi, Vikas Chandra, Yunyang Xiong, Mohamed Elhoseiny.
  <br>
  <em>arxiv</em>, 2023
  <br>
  <div>
    <a href="https://minigpt-v2.github.io/">[project]</a>
    <a href="https://arxiv.org/abs/2310.09478">[paper]</a>
    <a href="https://github.com/Vision-CAIR/MiniGPT-4">[code]</a>
    <a href="https://huggingface.co/spaces/Vision-CAIR/MiniGPT-v2">[huggingface demo]</a>
  </div>  
</td>
</tr>  


<tr>
<td style="padding:0px;width:30%;vertical-align:middle">
  <img src="../images/rsvlm.png" height="100%" width="100%" style="border-style: none">
</td>
<td style="padding:20px;width:70%;vertical-align:middle">  
  <h3>Vision-Language Models in Remote Sensing: Current Progress and Future Trends.
  </h3>
  <b>Xiang Li*</b>, Congcong Wen, Yuan Hu, Zhengpeng Yuan, Xiao Xiang Zhu.
  <br>
  <em>IEEE Geoscience and Remote Sensing Magazine (GRSM)</em>, 2024
  <br>
  <div>
    <!-- <a href="https://minigpt-4.github.io/">[project]</a> -->
    <a href="https://arxiv.org/abs/2305.05726">[paper]</a>
    <!-- <a href="https://github.com/Vision-CAIR/MiniGPT-4">[code]</a> -->
    <!-- <a href="https://huggingface.co/spaces/Vision-CAIR/minigpt4">[huggingface demo]</a> -->
  </div>  
</td>
</tr> 


<tr>
<td style="padding:0px;width:30%;vertical-align:middle">
  <img src="../images/rsgpt.png" height="100%" width="100%" style="border-style: none">
</td>
<td style="padding:20px;width:70%;vertical-align:middle">  
  <h3>RSGPT: A Remote Sensing Vision Language Model and Benchmark.</h3>
  Yuan Hu, Jianlong Yuan, Congcong Wen, Xiaonan Lu, <b>Xiang Li*</b>.
  <br>
  <em>arxiv</em>, 2023
  <br>
  <div>
    <a href="https://github.com/Lavender105/RSGPT">[project]</a>
    <a href="https://arxiv.org/abs/2307.15266">[paper]</a>
    <a href="https://github.com/Lavender105/RSGPT">[code]</a>
  </div>  
</td>
</tr>  

<tr>
<td style="padding:0px;width:30%;vertical-align:middle">
  <img src="../images/rsclip.png" height="100%" width="100%" style="border-style: none">
</td>
<td style="padding:20px;width:70%;vertical-align:middle">  
  <h3>RS-CLIP: Zero Shot Remote Sensing Scene Classification via Contrastive Vision-Language Supervision.</h3>
  <b>Xiang Li</b>, Xiang Li, Congcong Wen, Nan Zhou.
  <br>
  <em>International Journal of Applied Earth Observation and Geoinformation (JAG)</em>, 2023
  <br>
  <div>
    <a href="https://github.com/lx709/RS-CLIP">[project]</a>
    <a href="https://www.sciencedirect.com/science/article/pii/S1569843223003217">[paper]</a>
    <a href="https://github.com/lx709/RS-CLIP">[code]</a>
  </div>  
</td>
</tr>  



<tr>
<td style="padding:0px;width:30%;vertical-align:middle">
  <img src="../images/fsodm.png" height="100%" width="100%" style="border-style: none">
</td>
<td style="padding:20px;width:70%;vertical-align:middle">  
  <h3>Few-shot Object Detection on Remote Sensing Images.
  </h3>
  <b>Xiang Li<sup>#</sup></b>, Jingyu Deng<sup>#</sup>, Yi Fang.
  <br>
  <em>TGRS</em>, 2021
  <br>
  <div>
    <!-- <a href="https://github.com/xiaoqian-shen/MoStGAN-V/">[project]</a> -->
    <a href="https://ieeexplore.ieee.org/document/9362267">[paper]</a>
    <a href="https://github.com/lixiang-ucas/FSODM">[code]</a>
    <!-- <a href="https://huggingface.co/spaces/Vision-CAIR/minigpt4">[huggingface demo]</a> -->
  </div>  
</td>
</tr> 



<tr>
<td style="padding:0px;width:30%;vertical-align:middle">
  <img src="../images/dancenet.png" height="100%" width="100%" style="border-style: none">
</td>
<td style="padding:20px;width:70%;vertical-align:middle">  
  <h3>Density-Aware Convolutional Networks with Context Encoding for Airborne LiDAR Point Cloud Classification.
  </h3>
  <b>Xiang Li</b>, Lingjing Wang, Mingyang Wang, Congcong Wen, Nan Zhou, Yi Fang.
  <br>
  <em>ISPRS JPRS</em>, 2021
  <br>
  <div>
    <!-- <a href="https://github.com/xiaoqian-shen/MoStGAN-V/">[project]</a> -->
    <a href="https://arxiv.org/abs/1910.05909">[paper]</a>
    <!-- <a href="https://github.com/xiaoqian-shen/MoStGAN-V">[code]</a> -->
    <!-- <a href="https://huggingface.co/spaces/Vision-CAIR/minigpt4">[huggingface demo]</a> -->
  </div>  
</td>
</tr> 



<tr>
<td style="padding:0px;width:30%;vertical-align:middle">
  <img src="../images/tpnet.png" height="100%" width="100%" style="border-style: none">
</td>
<td style="padding:20px;width:70%;vertical-align:middle">  
  <h3>Topology Constrained Shape Correspondence.
  </h3>
  <b>Xiang Li<sup>#</sup></b>, Congcong Wen<sup>#</sup>, Lingjing Wang, Yi Fang.
  <br>
  <em>TVCG</em>, 2020
  <br>
  <div>
    <!-- <a href="https://github.com/xiaoqian-shen/MoStGAN-V/">[project]</a> -->
    <a href="https://ieeexplore.ieee.org/document/9091324">[paper]</a>
    <a href="https://github.com/lixiang-ucas/TP-Net">[code]</a>
    <!-- <a href="https://huggingface.co/spaces/Vision-CAIR/minigpt4">[huggingface demo]</a> -->
  </div>  
</td>
</tr> 


<tr>
<td style="padding:0px;width:30%;vertical-align:middle">
  <img src="../images/wps-net.png" height="100%" width="100%" style="border-style: none">
</td>
<td style="padding:20px;width:70%;vertical-align:middle">  
  <h3>Few-shot Learning of Part-specific Probability Space for 3D Shape Segmentation.
  </h3>
  Lingjing Wang<sup>#</sup>, <b>Xiang Li<sup>#</sup></b>, Yi Fang.
  <br>
  <em>CVPR</em>, 2020
  <br>
  <div>
    <!-- <a href="https://github.com/xiaoqian-shen/MoStGAN-V/">[project]</a> -->
    <a href="https://paperswithcode.com/paper/few-shot-learning-of-part-specific">[paper]</a>
    <a href="https://github.com/Lingjing324/Few-Shot-Learning-of-Part-Specific-Probability-Space-for-3D-Shape-Segmentation">[code]</a>
    <!-- <a href="https://huggingface.co/spaces/Vision-CAIR/minigpt4">[huggingface demo]</a> -->
  </div>  
</td>
</tr> 



<tr>
<td style="padding:0px;width:30%;vertical-align:middle">
  <img src="../images/d-fcn.png" height="100%" width="100%" style="border-style: none">
</td>
<td style="padding:20px;width:70%;vertical-align:middle">  
  <h3>Directionally Constrained Fully Convolutional Neural Network For Airborne Lidar Point Cloud Classification.
  </h3>
  Congcong Wen, Lina Yang, Ling Peng, <b>Xiang Li*</b>
  <br>
  <em>ISPRS JPRS</em>, 2020
  <br>
  <div>
    <!-- <a href="https://github.com/xiaoqian-shen/MoStGAN-V/">[project]</a> -->
    <a href="https://authors.elsevier.com/a/1abO93I9x1cfvT">[paper]</a>
    <a href="https://https://github.com/lx709/D-FCN">[code]</a>
    <!-- <a href="https://huggingface.co/spaces/Vision-CAIR/minigpt4">[huggingface demo]</a> -->
  </div>  
</td>
</tr> 


</table>


Honors and Awards
======
* Outstanding Reviewer for ICCV 2021.
* Postdoc Non-travel Award, NYUAD 2020 & 2021.
* National Scholarship, University of Chinese Academy of Sciences, 2018
* China Scholarship Council (CSC) scholarship, 2017
* Director's Fund of RADI, 2017
* Seagate Scholarship, Wuhan University, 2012
* National Scholarship, Wuhan University, 2011



