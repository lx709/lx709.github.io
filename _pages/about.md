---
permalink: /
title: "About me ([Curriculum Vitae](https://xiangli.ac.cn/files/xiang_en.pdf))"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

<!-- <p align="center">
  <img src="https://lx709.github.io/images/lx.jpg?raw=true" alt="Photo" style="width: 100px;"/> 
</p> -->

I am a Postdoc Researcher at VisionCAIR lab, KAUST. Prior to joining KAIST, I was a Postdoc Researcher at MMVC Lab, NYU Tandon and NYU Abu Dhabi. I received my Bachelor’s degree from Wuhan University in 2014 and my Ph.D. degree from the RADI, CAS in 2019. My research interests lie in computer vision and remote sensing.

Honors and Awards
======
* Outstanding Reviewer for ICCV 2021.
* Postdoc Non-travel Award, NYUAD 2020 & 2021.
* National Scholarship, University of Chinese Academy of Sciences, 2018
* Excellent research paper award, Institute of Remote Sensing and Digital Earth, CAS, 2018
* China Scholarship Council scholarship, 2017
* Presidential Foundation of RADI, 2017
* Seagate Scholarship, Wuhan University, 2012
* National Scholarship, Wuhan University, 2011

Academic Service
======
* Journal/Conference Review: 
  * ISPRS JPRS, TGRS, GRSL, JSTAR, IJGI, IJDI, JARS, Sensors, AIRQ.
  * TIP, TVCG, TEVC, TBD, CVIU, NeuroComputing, PRL.
  * CVPR (2022/2023/2024), ICCV (2021/2023), ECCV (2022), NeruIPS (2023), ICLR (2024), AAAI (2022/2023/2024), BMVC (2020/2021/2022), WACV (2022/2023)

<h2><span>Selected Publications</span></h2>
[# denotes equal contribution, * denotes corresponding author]
<table cellspacing="0" cellpadding="0">



<tr>
<td style="padding:0px;width:30%;vertical-align:middle">
  <img src="../images/minigpt-v2.png" height="100%" width="100%" style="border-style: none">
</td>
<td style="padding:20px;width:70%;vertical-align:middle">  
  <h3>MiniGPT-v2: Large Language Model as a Unified Interface for Vision-Language Multi-task Learning.</h3>
  Jun Chen, Deyao Zhu, Xiaoqian Shen, <b>Xiang Li</b>, Zechun Liu, Pengchuan Zhang, Raghuraman Krishnamoorthi, Vikas Chandra, Yunyang Xiong, Mohamed Elhoseiny.
  <br>
  <em>arxiv</em>, 2023
  <br>
  <div>
    <a href="https://minigpt-v2.github.io/">[project]</a>
    <a href="https://arxiv.org/abs/2310.09478">[paper]</a>
    <a href="https://github.com/Vision-CAIR/MiniGPT-4">[code]</a>
    <a href="https://huggingface.co/spaces/Vision-CAIR/MiniGPT-v2">[huggingface demo]</a>
  </div>  
</td>
</tr>  


<tr>
<td style="padding:0px;width:30%;vertical-align:middle">
  <img src="../images/minigpt4.png" height="100%" width="100%" style="border-style: none">
</td>
<td style="padding:20px;width:70%;vertical-align:middle">  
  <h3>MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models.</h3>
  Deyao Zhu, Jun Chen, Xiaoqian Shen, <b>Xiang Li</b>, Mohamed Elhoseiny.
  <br>
  <em>arxiv</em>, 2023
  <br>
  <div>
    <a href="https://minigpt-4.github.io/">[project]</a>
    <a href="https://arxiv.org/abs/2304.10592">[paper]</a>
    <a href="https://github.com/Vision-CAIR/MiniGPT-4">[code]</a>
    <a href="https://huggingface.co/spaces/Vision-CAIR/minigpt4">[huggingface demo]</a>
  </div>  
</td>
</tr>  

<tr>
<td style="padding:0px;width:30%;vertical-align:middle">
  <img src="../images/rsvlm.png" height="100%" width="100%" style="border-style: none">
</td>
<td style="padding:20px;width:70%;vertical-align:middle">  
  <h3>Vision-Language Models in Remote Sensing: Current Progress and Future Trends.
  </h3>
  <b>Xiang Li*</b>, Congcong Wen, Yuan Hu, Zhengpeng Yuan, Xiao Xiang Zhu.
  <br>
  <em>under review</em>, 2023
  <br>
  <div>
    <!-- <a href="https://minigpt-4.github.io/">[project]</a> -->
    <a href="https://arxiv.org/abs/2305.05726">[paper]</a>
    <!-- <a href="https://github.com/Vision-CAIR/MiniGPT-4">[code]</a> -->
    <!-- <a href="https://huggingface.co/spaces/Vision-CAIR/minigpt4">[huggingface demo]</a> -->
  </div>  
</td>
</tr> 


<tr>
<td style="padding:0px;width:30%;vertical-align:middle">
  <img src="../images/jcr_net.png" height="100%" width="100%" style="border-style: none">
</td>
<td style="padding:20px;width:70%;vertical-align:middle">  
  <h3>Unsupervised Category-Specific Partial Point Set Registration via Joint Shape Completion and Registration.
  </h3>
  <b>Xiang Li</b>, Lingjing Wang, Yi Fang.
  <br>
  <em>TVCG</em>, 2022
  <br>
  <div>
    <!-- <a href="https://github.com/xiaoqian-shen/MoStGAN-V/">[project]</a> -->
    <a href="https://arxiv.org/abs/2009.05290">[paper]</a>
    <!-- <a href="https://github.com/xiaoqian-shen/MoStGAN-V">[code]</a> -->
    <!-- <a href="https://huggingface.co/spaces/Vision-CAIR/minigpt4">[huggingface demo]</a> -->
  </div>  
</td>
</tr> 


<tr>
<td style="padding:0px;width:30%;vertical-align:middle">
  <img src="../images/fsodm.png" height="100%" width="100%" style="border-style: none">
</td>
<td style="padding:20px;width:70%;vertical-align:middle">  
  <h3>Few-shot Object Detection on Remote Sensing Images.
  </h3>
  <b>Xiang Li<sup>#</sup></b>, Jingyu Deng<sup>#</sup>, Yi Fang.
  <br>
  <em>TGRS</em>, 2021
  <br>
  <div>
    <!-- <a href="https://github.com/xiaoqian-shen/MoStGAN-V/">[project]</a> -->
    <a href="https://ieeexplore.ieee.org/document/9362267">[paper]</a>
    <a href="https://github.com/lixiang-ucas/FSODM">[code]</a>
    <!-- <a href="https://huggingface.co/spaces/Vision-CAIR/minigpt4">[huggingface demo]</a> -->
  </div>  
</td>
</tr> 



<tr>
<td style="padding:0px;width:30%;vertical-align:middle">
  <img src="../images/dancenet.png" height="100%" width="100%" style="border-style: none">
</td>
<td style="padding:20px;width:70%;vertical-align:middle">  
  <h3>Density-Aware Convolutional Networks with Context Encoding for Airborne LiDAR Point Cloud Classification.
  </h3>
  <b>Xiang Li</b>, Lingjing Wang, Mingyang Wang, Congcong Wen, Nan Zhou, Yi Fang.
  <br>
  <em>ISPRS JPRS</em>, 2021
  <br>
  <div>
    <!-- <a href="https://github.com/xiaoqian-shen/MoStGAN-V/">[project]</a> -->
    <a href="https://arxiv.org/abs/1910.05909">[paper]</a>
    <!-- <a href="https://github.com/xiaoqian-shen/MoStGAN-V">[code]</a> -->
    <!-- <a href="https://huggingface.co/spaces/Vision-CAIR/minigpt4">[huggingface demo]</a> -->
  </div>  
</td>
</tr> 



<tr>
<td style="padding:0px;width:30%;vertical-align:middle">
  <img src="../images/tpnet.png" height="100%" width="100%" style="border-style: none">
</td>
<td style="padding:20px;width:70%;vertical-align:middle">  
  <h3>Topology Constrained Shape Correspondence.
  </h3>
  <b>Xiang Li<sup>#</sup></b>, Congcong Wen<sup>#</sup>, Lingjing Wang, Yi Fang.
  <br>
  <em>TVCG</em>, 2020
  <br>
  <div>
    <!-- <a href="https://github.com/xiaoqian-shen/MoStGAN-V/">[project]</a> -->
    <a href="https://ieeexplore.ieee.org/document/9091324">[paper]</a>
    <a href="https://github.com/lixiang-ucas/TP-Net">[code]</a>
    <!-- <a href="https://huggingface.co/spaces/Vision-CAIR/minigpt4">[huggingface demo]</a> -->
  </div>  
</td>
</tr> 


<tr>
<td style="padding:0px;width:30%;vertical-align:middle">
  <img src="../images/wps-net.png" height="100%" width="100%" style="border-style: none">
</td>
<td style="padding:20px;width:70%;vertical-align:middle">  
  <h3>Few-shot Learning of Part-specific Probability Space for 3D Shape Segmentation.
  </h3>
  Lingjing Wang<sup>#</sup>, <b>Xiang Li<sup>#</sup></b>, Yi Fang.
  <br>
  <em>CVPR</em>, 2020
  <br>
  <div>
    <!-- <a href="https://github.com/xiaoqian-shen/MoStGAN-V/">[project]</a> -->
    <a href="https://paperswithcode.com/paper/few-shot-learning-of-part-specific">[paper]</a>
    <a href="https://github.com/Lingjing324/Few-Shot-Learning-of-Part-Specific-Probability-Space-for-3D-Shape-Segmentation">[code]</a>
    <!-- <a href="https://huggingface.co/spaces/Vision-CAIR/minigpt4">[huggingface demo]</a> -->
  </div>  
</td>
</tr> 



<tr>
<td style="padding:0px;width:30%;vertical-align:middle">
  <img src="../images/d-fcn.png" height="100%" width="100%" style="border-style: none">
</td>
<td style="padding:20px;width:70%;vertical-align:middle">  
  <h3>Directionally Constrained Fully Convolutional Neural Network For Airborne Lidar Point Cloud Classification.
  </h3>
  Congcong Wen, Lina Yang, Ling Peng, <b>Xiang Li*</b>
  <br>
  <em>ISPRS JPRS</em>, 2020
  <br>
  <div>
    <!-- <a href="https://github.com/xiaoqian-shen/MoStGAN-V/">[project]</a> -->
    <a href="https://authors.elsevier.com/a/1abO93I9x1cfvT">[paper]</a>
    <a href="https://https://github.com/lx709/D-FCN">[code]</a>
    <!-- <a href="https://huggingface.co/spaces/Vision-CAIR/minigpt4">[huggingface demo]</a> -->
  </div>  
</td>
</tr> 


</table>



