---
permalink: /
title: "About me ([Curriculum Vitae](https://xiangli.ac.cn/files/xiang_en.pdf))"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---


Dr. Xiang Li is a Lecturer (Assistant Professor) in the School of Computer Science at the University of Bristol and a faculty member of the MAVI group. His research focuses on multimodal large language models, computer vision, and remote sensing. Dr. Li has published over 50 papers in top conferences and journals in computer vision and remote sensing, such as CVPR, ICCV, NeurIPS, TVCG, and TGRS, with <a href="https://scholar.google.com/citations?user=4Apl5FgAAAAJ&hl=en"><img src="https://img.shields.io/badge/Google%20Scholar-7900%20citations-9cf?logo=Google%20Scholar" /></a> citations on Google Scholar and an h-index of 28. Dr Li contributed to the well-known <a href="https://github.com/Vision-CAIR/MiniGPT-4">MiniGPT-4</a><img src="https://img.shields.io/github/stars/Vision-CAIR/MiniGPT-4?style=social" /> project, with over 3000 citations on Google Scholar. Dr. Li co-organized the 1st and 2nd workshops on [Compositional 3D Vision](https://3dcompat-dataset.org/workshop/C3DV24/) and the 3DCoMPaT dataset challenge at IEEE CVPR 2023 and 2024. Additionally, he serves as Guest Editor for the special issue [Vision-Language Models in Remote Sensing](https://www.grss-ieee.org/wp-content/uploads/2024/10/call-for-papers_GRSM_SI_VLM.pdf) at IEEE GRSM. Dr Li received the IEEE GRSS Early Career Award 2025 for his high-impact research in computer vision methods in remote sensing applications.

ðŸ”¥Hiring PhD students and interns: Welcome enthusiastic students passionate about multimodal LLMs, 3D vision, and remote sensing to apply for PhD positions and internships.

ðŸ”¥ News
* [09/2025] I joined the University of Bristol as a Lecturer in Computer Vision.
* [08/2025] I received the **IEEE GRSS Early Career Award** 2025.
* [08/2025] [3DCoMPaT++](https://arxiv.org/abs/2310.18511) was accepted for publication at **IEEE TPAMI**.
* [07/2025] I am honored to serve as the Guest Editor for the Special Issue titled "Advancing Geospatial Image Perception and Understanding Under Challenging Real-World Conditions" in the journal Geo-spatial Information Science (**GSIS**).
* [07/2025] One paper on visual grounding [GeoGound](https://github.com/VisionXLab/GeoGround) was accepted for publication at **ICCV 2025 SEA workshop**.
* [06/2025] One paper on few-shot oriented object detection ([FOMC](https://ieeexplore.ieee.org/document/11030667/)) was accepted by **IEEE TGRS**. 
* [06/2025] I joined ELLIS, a pan-European AI network of excellence. 
* [05/2025] We released [**REOBench**](https://github.com/lx709/REOBench): A benchmark for evaluating the robustness of Earth observation foundation models.
* [05/2025] I gave a seminar talk on "Large Vision Language Models in Remote Sensing: Datasets and Models" at the **Data Assimilation Research Center**.
* [04/2025] Stable-SPAM was accepted for publication at **ICLR 2025 Workshop SCOPE**.
* [03/2025] [**RSGPT**](https://github.com/Lavender105/RSGPT) was accepted for publication at **ISPRS JPRS**. RSGPT is the first attempt at GPT-based MLLMs in remote sensing.
* [03/2025] [**Stable-SPAM**](https://arxiv.org/abs/2502.17055) was accepted for publication at **COLM**. 
* [12/2024] Opening position: One fully funded PhD position in AI for Bioversity (closed).
* [10/2024] I joined the University of Reading as a Lecturer in Computer Science.
* [09/2024] Two papers accepted at NeruIPS 2024 Datasets and Benchmarks Track [VRSBench](https://github.com/lx709/VRSBench) and [3DCoMPaT200](https://github.com/3DCoMPaT200/3DCoMPaT200).
* [07/2024] One paper on 3D LLM (**Uni3DL**) was accepted by **ECCV 2024**.
* [07/2024] I will serve as the Guest Editor for the special issue "[Vision-Language Models in Remote Sensing](https://www.grss-ieee.org/wp-content/uploads/2024/10/call-for-papers_GRSM_SI_VLM.pdf)" at **IEEE GRSM**.
* [06/2024] I'm co-organizing the [C3DV 2024](https://3dcompat-dataset.org/workshop/C3DV24/): 2nd Workshop on Compositional 3D Vision at CVPR 2024.
* [05/2024] One paper on few-shot object detection ([InfRS](https://ieeexplore.ieee.org/document/10706935/)) was accepted by **IEEE TGRS**.
* [05/2024] One paper on CO<sub>2</sub> mapping was published at **JAG**.
* [04/2024] Our survey paper [Vision-Language Models in Remote Sensing](https://ieeexplore.ieee.org/document/10506064) was published at IEEE GRSM.
* [03/2024] I gave a talk "AI for Earth Observation" at Prof. [Matthew McCabe](https://www.kaust.edu.sa/en/study/faculty/matthew-mccabe)'s lab.
* [11/2023] [**RS-CLIP**](https://github.com/lx709/RS-CLIP) was published at **JAG**.
* [10/2023] We released [**MiniGPT-v2**](https://minigpt-4.github.io/).
* [07/2023] We released [**RSGPT**](https://github.com/Lavender105/RSGPT), the first GPT-based multimodal LLM in remote sensing.
* [05/2023] We released the survey paper [**Vision-Language Models in Remote Sensing**](https://arxiv.org/abs/2305.05726).
* [04/2023] We released [**MiniGPT-4**](https://minigpt-4.github.io/).

<h2><span>Selected Publications</span></h2>
[# denotes equal contribution, * denotes corresponding author]
<table cellspacing="0" cellpadding="0">

<tr>
<td style="padding:0px;width:30%;vertical-align:middle">
  <img src="../images/minigpt4.png" height="100%" width="100%" style="border-style: none">
</td>
<td style="padding:20px;width:70%;vertical-align:middle">  
  <h3> MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models.</h3>
  Deyao Zhu, Jun Chen, Xiaoqian Shen, <b>Xiang Li</b>, Mohamed Elhoseiny.
  <br>
  <em>ICLR</em>, 2024
  <br>
  <div>
    <a href="https://minigpt-4.github.io/">[project]</a>
    <a href="https://arxiv.org/abs/2304.10592">[paper]</a>
    <a href="https://github.com/Vision-CAIR/MiniGPT-4">[code]</a>
    <a href="https://huggingface.co/spaces/Vision-CAIR/minigpt4">[huggingface demo]</a>
  </div>  
</td>
</tr>  

<tr>
<td style="padding:0px;width:30%;vertical-align:middle">
  <img src="../images/minigpt-v2.png" height="100%" width="100%" style="border-style: none">
</td>
<td style="padding:20px;width:70%;vertical-align:middle">  
  <h3>MiniGPT-v2: Large Language Model as a Unified Interface for Vision-Language Multi-task Learning.</h3>
  Jun Chen, Deyao Zhu, Xiaoqian Shen, <b>Xiang Li</b>, Zechun Liu, Pengchuan Zhang, Raghuraman Krishnamoorthi, Vikas Chandra, Yunyang Xiong, Mohamed Elhoseiny.
  <br>
  <em>arxiv</em>, 2023
  <br>
  <div>
    <a href="https://minigpt-v2.github.io/">[project]</a>
    <a href="https://arxiv.org/abs/2310.09478">[paper]</a>
    <a href="https://github.com/Vision-CAIR/MiniGPT-4">[code]</a>
    <a href="https://huggingface.co/spaces/Vision-CAIR/MiniGPT-v2">[huggingface demo]</a>
  </div>  
</td>
</tr>  


<tr>
<td style="padding:0px;width:30%;vertical-align:middle">
  <img src="../images/rsgpt.png" height="100%" width="100%" style="border-style: none">
</td>
<td style="padding:20px;width:70%;vertical-align:middle">  
  <h3>RSGPT: A Remote Sensing Vision Language Model and Benchmark.</h3>
  Yuan Hu, Jianlong Yuan, Congcong Wen, Xiaonan Lu, <b>Xiang Li*</b>.
  <br>
  <em>ISPRS JPRS</em>, 2025 (arXiv 2023)
  <br>
  <div>
    <a href="https://github.com/Lavender105/RSGPT">[project]</a>
    <a href="https://arxiv.org/abs/2307.15266">[paper]</a>
    <a href="https://github.com/Lavender105/RSGPT">[code]</a>
  </div>  
</td>
</tr>  


<tr>
<td style="padding:0px;width:30%;vertical-align:middle">
  <img src="../images/rsvlm.png" height="100%" width="100%" style="border-style: none">
</td>
<td style="padding:20px;width:70%;vertical-align:middle">  
  <h3>Vision-Language Models in Remote Sensing: Current Progress and Future Trends.
  </h3>
  <b>Xiang Li*</b>, Congcong Wen, Yuan Hu, Zhengpeng Yuan, Xiao Xiang Zhu.
  <br>
  <em>IEEE Geoscience and Remote Sensing Magazine (GRSM)</em>, 2024
  <br>
  <div>
    <!-- <a href="https://minigpt-4.github.io/">[project]</a> -->
    <a href="https://arxiv.org/abs/2305.05726">[paper]</a>
    <!-- <a href="https://github.com/Vision-CAIR/MiniGPT-4">[code]</a> -->
    <!-- <a href="https://huggingface.co/spaces/Vision-CAIR/minigpt4">[huggingface demo]</a> -->
  </div>  
</td>
</tr> 



<tr>
<td style="padding:0px;width:30%;vertical-align:middle">
  <img src="../images/rsclip.png" height="100%" width="100%" style="border-style: none">
</td>
<td style="padding:20px;width:70%;vertical-align:middle">  
  <h3>RS-CLIP: Zero Shot Remote Sensing Scene Classification via Contrastive Vision-Language Supervision.</h3>
  <b>Xiang Li</b>, Xiang Li, Congcong Wen, Nan Zhou.
  <br>
  <em>International Journal of Applied Earth Observation and Geoinformation (JAG)</em>, 2023
  <br>
  <div>
    <a href="https://github.com/lx709/RS-CLIP">[project]</a>
    <a href="https://www.sciencedirect.com/science/article/pii/S1569843223003217">[paper]</a>
    <a href="https://github.com/lx709/RS-CLIP">[code]</a>
  </div>  
</td>
</tr>  



<tr>
<td style="padding:0px;width:30%;vertical-align:middle">
  <img src="../images/fsodm.png" height="100%" width="100%" style="border-style: none">
</td>
<td style="padding:20px;width:70%;vertical-align:middle">  
  <h3>Few-shot Object Detection on Remote Sensing Images.
  </h3>
  <b>Xiang Li<sup>#</sup></b>, Jingyu Deng<sup>#</sup>, Yi Fang.
  <br>
  <em>TGRS</em>, 2021
  <br>
  <div>
    <!-- <a href="https://github.com/xiaoqian-shen/MoStGAN-V/">[project]</a> -->
    <a href="https://ieeexplore.ieee.org/document/9362267">[paper]</a>
    <a href="https://github.com/lixiang-ucas/FSODM">[code]</a>
    <!-- <a href="https://huggingface.co/spaces/Vision-CAIR/minigpt4">[huggingface demo]</a> -->
  </div>  
</td>
</tr> 




<tr>
<td style="padding:0px;width:30%;vertical-align:middle">
  <img src="../images/wps-net.png" height="100%" width="100%" style="border-style: none">
</td>
<td style="padding:20px;width:70%;vertical-align:middle">  
  <h3>Few-shot Learning of Part-specific Probability Space for 3D Shape Segmentation.
  </h3>
  Lingjing Wang<sup>#</sup>, <b>Xiang Li<sup>#</sup></b>, Yi Fang.
  <br>
  <em>CVPR</em>, 2020
  <br>
  <div>
    <!-- <a href="https://github.com/xiaoqian-shen/MoStGAN-V/">[project]</a> -->
    <a href="https://paperswithcode.com/paper/few-shot-learning-of-part-specific">[paper]</a>
    <a href="https://github.com/Lingjing324/Few-Shot-Learning-of-Part-Specific-Probability-Space-for-3D-Shape-Segmentation">[code]</a>
    <!-- <a href="https://huggingface.co/spaces/Vision-CAIR/minigpt4">[huggingface demo]</a> -->
  </div>  
</td>
</tr> 



<tr>
<td style="padding:0px;width:30%;vertical-align:middle">
  <img src="../images/d-fcn.png" height="100%" width="100%" style="border-style: none">
</td>
<td style="padding:20px;width:70%;vertical-align:middle">  
  <h3>Directionally Constrained Fully Convolutional Neural Network For Airborne Lidar Point Cloud Classification.
  </h3>
  Congcong Wen, Lina Yang, Ling Peng, <b>Xiang Li*</b>
  <br>
  <em>ISPRS JPRS</em>, 2020
  <br>
  <div>
    <!-- <a href="https://github.com/xiaoqian-shen/MoStGAN-V/">[project]</a> -->
    <a href="https://authors.elsevier.com/a/1abO93I9x1cfvT">[paper]</a>
    <a href="https://https://github.com/lx709/D-FCN">[code]</a>
    <!-- <a href="https://huggingface.co/spaces/Vision-CAIR/minigpt4">[huggingface demo]</a> -->
  </div>  
</td>
</tr> 


</table>


Honors and Awards
======
* IEEE GRSS Early Career Award 2025
* Outstanding Reviewer for ICCV 2021.
* Postdoc Non-travel Award, NYUAD 2020 & 2021.
* National Scholarship, University of Chinese Academy of Sciences, 2018
* China Scholarship Council (CSC) scholarship, 2017
* Director's Fund of RADI, 2017
* Seagate Scholarship, Wuhan University, 2012
* National Scholarship, Wuhan University, 2011



